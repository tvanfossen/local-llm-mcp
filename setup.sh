#!/bin/bash

# File: ~/Projects/local-llm-mcp/setup.sh
# Setup script for Standardized Agent-Based Local LLM MCP Server
# Environment: Ubuntu 22.04, NVIDIA Driver 575, CUDA 12.9, RTX 1080ti (11GB VRAM)

set -e

echo "üöÄ Setting up Standardized Agent-Based Local LLM MCP Server..."
echo "üìã Target: Ubuntu 22.04 + NVIDIA Driver 575 + CUDA 12.9 + RTX 1080ti"
echo "üìÅ Location: ~/Projects/local-llm-mcp/ (Git repo)"

# Check CUDA
if ! command -v nvidia-smi &> /dev/null; then
    echo "‚ö†Ô∏è  Warning: nvidia-smi not found. Make sure you have NVIDIA drivers installed."
    exit 1
fi

CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | sed -n 's/.*CUDA Version: \([0-9]*\.[0-9]*\).*/\1/p')
echo "üîç Detected CUDA Version: $CUDA_VERSION"

if [[ "$CUDA_VERSION" < "12.0" ]]; then
    echo "‚ö†Ô∏è  Warning: CUDA version is less than 12.0. This setup is optimized for CUDA 12.9."
fi

# Create project directory structure
PROJECT_DIR="$HOME/Projects/local-llm-mcp"
echo "üìÅ Creating project at: $PROJECT_DIR"

# Initialize git repo if it doesn't exist
if [ ! -d "$PROJECT_DIR/.git" ]; then
    mkdir -p "$PROJECT_DIR"
    cd "$PROJECT_DIR"
    
    echo "üîß Initializing Git repository..."
    git init
    
    # Create initial .gitignore
    cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
venv/

# Model files (too large)
models/
*.gguf

# Generated content
workspaces/
logs/
state/

# Local config
.env
*.local

# IDE
.vscode/
.idea/

# OS
.DS_Store
EOF
    
    echo "üìù Created .gitignore"
else
    cd "$PROJECT_DIR"
    echo "‚úÖ Git repository already exists"
fi

# Create folder structure
echo "üìÅ Creating standardized directory structure..."
mkdir -p schemas
mkdir -p agents
mkdir -p models
mkdir -p state
mkdir -p workspaces
mkdir -p logs

# Create Python virtual environment
echo "üì¶ Creating Python virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies optimized for CUDA 12.9
echo "üì• Installing dependencies for CUDA 12.9..."
pip install --upgrade pip

# Install llama-cpp-python with CUDA 12.x support
echo "üîß Compiling llama-cpp-python with CUDA 12.9 support..."
export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12"
export FORCE_CMAKE=1
export CUDACXX=/usr/local/cuda-12/bin/nvcc

# Verify CUDA toolkit path
if [ -d "/usr/local/cuda-12" ]; then
    echo "‚úÖ CUDA 12.x toolkit found at /usr/local/cuda-12"
elif [ -d "/usr/local/cuda" ]; then
    echo "‚úÖ CUDA toolkit found at /usr/local/cuda"
    export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda"
    export CUDACXX=/usr/local/cuda/bin/nvcc
else
    echo "‚ö†Ô∏è  CUDA toolkit not found. Installing CPU version only..."
    export CMAKE_ARGS=""
    export CUDACXX=""
fi

pip install llama-cpp-python[cuda]

# Install other dependencies
pip install mcp pydantic

# Create requirements.txt with JSON schema support
echo "üìÑ Creating requirements.txt..."
cat > requirements.txt << 'EOF'
mcp>=0.9.0
llama-cpp-python[cuda]>=0.2.0
pydantic>=2.0.0
EOF

# Create schemas module
echo "üîß Creating JSON schema module..."
touch schemas/__init__.pylocal-llm-mcp/setup.sh
# Setup script for Simple Agent-Based Local LLM MCP Server
# Environment: Ubuntu 22.04, NVIDIA Driver 575, CUDA 12.9, RTX 1080ti (11GB VRAM)

set -e

echo "üöÄ Setting up Simple Agent-Based Local LLM MCP Server..."
echo "üìã Target: Ubuntu 22.04 + NVIDIA Driver 575 + CUDA 12.9 + RTX 1080ti"

# Check CUDA
if ! command -v nvidia-smi &> /dev/null; then
    echo "‚ö†Ô∏è  Warning: nvidia-smi not found. Make sure you have NVIDIA drivers installed."
    exit 1
fi

CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | sed -n 's/.*CUDA Version: \([0-9]*\.[0-9]*\).*/\1/p')
echo "üîç Detected CUDA Version: $CUDA_VERSION"

if [[ "$CUDA_VERSION" < "12.0" ]]; then
    echo "‚ö†Ô∏è  Warning: CUDA version is less than 12.0. This setup is optimized for CUDA 12.9."
fi

# Create project directory
PROJECT_DIR="$HOME/local-llm-mcp"
mkdir -p "$PROJECT_DIR"
cd "$PROJECT_DIR"

# Create folder structure
echo "üìÅ Creating directory structure..."
mkdir -p agents
mkdir -p models
mkdir -p state
mkdir -p workspaces
mkdir -p logs

# Create Python virtual environment
echo "üì¶ Creating Python virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies optimized for CUDA 12.9
echo "üì• Installing dependencies for CUDA 12.9..."
pip install --upgrade pip

# Install llama-cpp-python with CUDA 12.x support
echo "üîß Compiling llama-cpp-python with CUDA 12.9 support..."
export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12"
export FORCE_CMAKE=1
export CUDACXX=/usr/local/cuda-12/bin/nvcc

# Verify CUDA toolkit path
if [ -d "/usr/local/cuda-12" ]; then
    echo "‚úÖ CUDA 12.x toolkit found at /usr/local/cuda-12"
elif [ -d "/usr/local/cuda" ]; then
    echo "‚úÖ CUDA toolkit found at /usr/local/cuda"
    export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda"
    export CUDACXX=/usr/local/cuda/bin/nvcc
else
    echo "‚ö†Ô∏è  CUDA toolkit not found. Installing CPU version only..."
    export CMAKE_ARGS=""
    export CUDACXX=""
fi

pip install llama-cpp-python[cuda]

# Install other dependencies
pip install mcp pydantic

# Create requirements.txt
echo "üìÑ Creating requirements.txt..."
cat > requirements.txt << 'EOF'
mcp>=0.9.0
llama-cpp-python[cuda]>=0.2.0
pydantic>=2.0.0
EOF

# Create agents/__init__.py
echo "ü§ñ Creating agent module..."
touch agents/__init__.py

cat > agents/base_agent.py << 'EOF'
# File: ~/local-llm-mcp/agents/base_agent.py
"""
Simple agent base class for the MCP server.
RULE: One agent per file, one file per agent.
"""

from dataclasses import dataclass
from typing import Dict, Any
from datetime import datetime, timezone

@dataclass
class SimpleAgent:
    """Minimal agent data structure - one agent manages one file only"""
    agent_id: str
    name: str
    description: str
    system_prompt: str
    managed_file: str  # Single file only
    context: str = ""
    created_at: str = ""
    last_active: str = ""
    
    def __post_init__(self):
        if not self.created_at:
            self.created_at = datetime.now(timezone.utc).isoformat()
        if not self.last_active:
            self.last_active = self.created_at
EOF

# Download model
echo "ü§ñ Downloading Qwen2.5-Coder 7B model..."
MODEL_URL="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf"
MODEL_FILE="models/qwen2.5-coder-7b-instruct.gguf"

if [ ! -f "$MODEL_FILE" ]; then
    echo "‚¨áÔ∏è  Downloading model (this may take a while)..."
    if command -v wget &> /dev/null; then
        wget -O "$MODEL_FILE" "$MODEL_URL"
    elif command -v curl &> /dev/null; then
        curl -L -o "$MODEL_FILE" "$MODEL_URL"
    else
        echo "‚ùå Neither wget nor curl found. Please download manually:"
        echo "   URL: $MODEL_URL"
        echo "   Save to: $PROJECT_DIR/$MODEL_FILE"
        exit 1
    fi
else
    echo "‚úÖ Model already exists: $MODEL_FILE"
fi

# Create Claude Code MCP configuration
CLAUDE_CONFIG_DIR="$HOME/.config/claude-code"
mkdir -p "$CLAUDE_CONFIG_DIR"

cat > "$CLAUDE_CONFIG_DIR/mcp.json" << EOF
{
  "mcpServers": {
    "local-agent-llm": {
      "command": "python3",
      "args": ["$PROJECT_DIR/local_llm_mcp_server.py"],
      "env": {
        "MODEL_PATH": "$PROJECT_DIR/$MODEL_FILE",
        "N_GPU_LAYERS": "-1",
        "N_CTX": "8192",
        "N_BATCH": "512", 
        "N_THREADS": "8",
        "USE_MMAP": "True",
        "VERBOSE": "False",
        "CUDA_VISIBLE_DEVICES": "0"
      }
    }
  }
}
EOF

# Create README
cat > README.md << 'EOF'
# Simple Agent-Based Local LLM MCP Server

A streamlined MCP server for Claude Code with persistent agents.

**CORE RULE: One agent per file, one file per agent.**

## Features

- **One-to-One Mapping**: Each agent manages exactly one file
- **No Conflicts**: File ownership is strictly enforced
- **CUDA 12.9 Optimized**: RTX 1080ti performance tuned
- **Context Persistence**: Agents remember conversations across sessions
- **Stdio Only**: Clean integration with Claude Code

## Quick Start

1. Run setup: `./setup.sh`
2. Start server: `./start.sh`
3. In Claude Code, agents will be available as tools

## Agent Workflow

```bash
# In Claude Code:
claude

# Create agents for different files:
"Use create_agent to make a database agent that manages schema.sql"
"Use create_agent to make an API agent that manages routes.py" 
"Use create_agent to make a frontend agent that manages index.html"

# Chat with specific agents:
"Use chat_with_agent to have the database agent design user tables"

# Have agents update their files:
"Use agent_manage_file to have the database agent create the schema.sql file"
```

## File Ownership Rules

- ‚úÖ One agent ‚Üí One file
- ‚úÖ One file ‚Üí One agent  
- ‚ùå Agent cannot access files it doesn't own
- ‚ùå Cannot create agent for file already owned
- ‚úÖ Deleting agent frees up the file for new agent

## Example Project Structure

```
Project/
‚îú‚îÄ‚îÄ schema.sql      ‚Üê Database Agent
‚îú‚îÄ‚îÄ models.py       ‚Üê ORM Agent  
‚îú‚îÄ‚îÄ routes.py       ‚Üê API Agent
‚îú‚îÄ‚îÄ index.html      ‚Üê Frontend Agent
‚îú‚îÄ‚îÄ styles.css      ‚Üê Styling Agent
‚îî‚îÄ‚îÄ tests.py        ‚Üê Testing Agent
```

Each file has one dedicated agent that understands its context and maintains its state.

## Directory Structure

- `agents/` - Agent classes
- `models/` - LLM model files  
- `state/` - Agent state persistence (includes file mappings)
- `workspaces/` - Per-agent workspaces with files
- `logs/` - Server and agent logs
EOF

# Create start script
cat > start.sh << 'EOF'
#!/bin/bash
# File: ~/local-llm-mcp/start.sh
cd "$(dirname "$0")"
source venv/bin/activate
echo "üöÄ Starting Simple Agent-Based LLM MCP Server..."
python3 local_llm_mcp_server.py
EOF

chmod +x start.sh

# Create test script  
cat > test.py << 'EOF'
#!/usr/bin/env python3
# File: ~/local-llm-mcp/test.py
"""Test script for the one-agent-per-file MCP server"""

import asyncio
import sys
from pathlib import Path

# Add project to path
sys.path.append(str(Path(__file__).parent))

from local_llm_mcp_server import SimpleLLMServer

async def test_server():
    print("üß™ Testing Simple Agent-Based LLM Server (One Agent Per File)...")
    
    server = SimpleLLMServer()
    
    print("ü§ñ Testing model loading...")
    if not server.load_model():
        print("‚ùå Model loading failed")
        return False
    
    print("‚úÖ Model loaded successfully!")
    
    # Test agent creation
    print("üîß Testing agent creation...")
    result1 = await server.create_agent({
        "name": "Database Agent",
        "description": "Manages the database schema file",
        "system_prompt": "You are a database specialist focused on SQL schema design.",
        "managed_file": "schema.sql",
        "initial_context": "Ready to design and maintain database schema."
    })
    
    print("‚úÖ First agent created!")
    print(f"Result: {result1.content[0].text[:100]}...")
    
    # Test file conflict prevention
    print("üö´ Testing file conflict prevention...")
    result2 = await server.create_agent({
        "name": "Another Agent", 
        "description": "Tries to manage same file",
        "system_prompt": "Another agent",
        "managed_file": "schema.sql"  # Same file - should fail
    })
    
    if "File Conflict" in result2.content[0].text:
        print("‚úÖ File conflict prevention works!")
    else:
        print("‚ùå File conflict prevention failed!")
        return False
    
    # Test different file - should succeed
    print("üìù Testing different file...")
    result3 = await server.create_agent({
        "name": "API Agent",
        "description": "Manages API routes",
        "system_prompt": "You are an API development specialist.",
        "managed_file": "routes.py"
    })
    
    print("‚úÖ Second agent with different file created!")
    
    # Test list agents
    print("üìã Testing agent listing...")
    list_result = await server.list_agents()
    if "File Ownership Map" in list_result.content[0].text:
        print("‚úÖ Agent listing shows file ownership!")
    
    print("\nüéâ All tests passed! One-agent-per-file rule enforced successfully.")
    print("üìä Key validations:")
    print("   ‚úÖ Model loads correctly")  
    print("   ‚úÖ Agent creation works")
    print("   ‚úÖ File conflicts are prevented") 
    print("   ‚úÖ Multiple agents with different files allowed")
    print("   ‚úÖ File ownership tracking works")
    
    return True

if __name__ == "__main__":
    asyncio.run(test_server())
EOF

chmod +x test.py

echo ""
echo "üéâ Setup complete!"
echo ""
echo "üìÅ Project directory: $PROJECT_DIR"
echo "ü§ñ Model location: $PROJECT_DIR/$MODEL_FILE"
echo "‚öôÔ∏è  Claude Code config: $CLAUDE_CONFIG_DIR/mcp.json"
echo ""
echo "üß™ To test the server:"
echo "   cd $PROJECT_DIR && source venv/bin/activate && python3 test.py"
echo ""
echo "üöÄ To start the MCP server:"
echo "   cd $PROJECT_DIR && ./start.sh"
echo ""
echo "üîß Claude Code should now detect your agent server!"
echo "   Available tools: create_agent, list_agents, chat_with_agent, etc."
echo ""
echo "‚ö†Ô∏è  **IMPORTANT RULE: One agent per file, one file per agent**"
echo "   - Each agent manages exactly one file"
echo "   - No file can be managed by multiple agents"
echo "   - This prevents conflicts and ensures clean ownership"

# GPU status
echo ""
echo "üñ•Ô∏è  GPU Status:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits | while read gpu; do
        echo "   GPU: $gpu MB"
    done
else
    echo "   ‚ö†Ô∏è  NVIDIA GPU tools not available"
fi

echo ""
echo "üí° RTX 1080ti (11GB) Usage Patterns:"
echo "   - schema.sql ‚Üí Database Agent"
echo "   - models.py ‚Üí ORM Agent" 
echo "   - routes.py ‚Üí API Agent"
echo "   - index.html ‚Üí Frontend Agent"
echo "   - styles.css ‚Üí Styling Agent"
echo "   - Each agent maintains context for its single file"#!/bin/bash

# File: ~/local-llm-mcp/setup.sh
# Setup script for Local LLM MCP Server
# Environment: Ubuntu 22.04, NVIDIA Driver 575, CUDA 12.9, RTX 1080ti (11GB VRAM)

set -e

echo "üöÄ Setting up Local LLM MCP Server for Claude Code..."
echo "üìã Target: Ubuntu 22.04 + NVIDIA Driver 575 + CUDA 12.9 + RTX 1080ti"

# Check if we're on the right system
if ! command -v nvidia-smi &> /dev/null; then
    echo "‚ö†Ô∏è  Warning: nvidia-smi not found. Make sure you have NVIDIA drivers installed."
    exit 1
fi

# Verify CUDA version
CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | sed -n 's/.*CUDA Version: \([0-9]*\.[0-9]*\).*/\1/p')
echo "üîç Detected CUDA Version: $CUDA_VERSION"

if [[ "$CUDA_VERSION" < "12.0" ]]; then
    echo "‚ö†Ô∏è  Warning: CUDA version is less than 12.0. This setup is optimized for CUDA 12.9."
fi

# Create project directory
PROJECT_DIR="$HOME/local-llm-mcp"
mkdir -p "$PROJECT_DIR"
cd "$PROJECT_DIR"

# Create Python virtual environment
echo "üì¶ Creating Python virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install dependencies optimized for CUDA 12.9
echo "üì• Installing dependencies for CUDA 12.9..."
pip install --upgrade pip

# Install llama-cpp-python with CUDA 12.x support
echo "üîß Compiling llama-cpp-python with CUDA 12.9 support..."
export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12"
export FORCE_CMAKE=1
export CUDACXX=/usr/local/cuda-12/bin/nvcc

# Verify CUDA toolkit path
if [ -d "/usr/local/cuda-12" ]; then
    echo "‚úÖ CUDA 12.x toolkit found at /usr/local/cuda-12"
elif [ -d "/usr/local/cuda" ]; then
    echo "‚úÖ CUDA toolkit found at /usr/local/cuda"
    export CMAKE_ARGS="-DLLAMA_CUDA=on -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda"
    export CUDACXX=/usr/local/cuda/bin/nvcc
else
    echo "‚ö†Ô∏è  CUDA toolkit not found. Installing CPU version only..."
    export CMAKE_ARGS=""
    export CUDACXX=""
fi

pip install llama-cpp-python[cuda]

# Install other dependencies
pip install mcp starlette uvicorn pydantic

# Create models directory
mkdir -p models

# Download recommended model (Qwen2.5-Coder 7B)
echo "ü§ñ Downloading Qwen2.5-Coder 7B model..."
echo "This may take a while (several GB download)..."

MODEL_URL="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf"
MODEL_FILE="models/qwen2.5-coder-7b-instruct.gguf"

if [ ! -f "$MODEL_FILE" ]; then
    if command -v wget &> /dev/null; then
        wget -O "$MODEL_FILE" "$MODEL_URL"
    elif command -v curl &> /dev/null; then
        curl -L -o "$MODEL_FILE" "$MODEL_URL"
    else
        echo "‚ùå Neither wget nor curl found. Please download the model manually:"
        echo "   URL: $MODEL_URL"
        echo "   Save to: $PROJECT_DIR/$MODEL_FILE"
        exit 1
    fi
else
    echo "‚úÖ Model already exists: $MODEL_FILE"
fi

# Create job processing directories
echo "üìÅ Creating job processing directories..."
mkdir -p jobs/{queue,processing,completed,failed}

# Create the MCP server script
echo "üìù Creating MCP server script..."
cat > local_llm_mcp_server.py << 'EOF'
# Copy the complete Python script from the artifact above
# This should be the full content of the local_llm_mcp_server.py file
EOF

# Make it executable
chmod +x local_llm_mcp_server.py

# Create Claude Code MCP configuration
CLAUDE_CONFIG_DIR="$HOME/.config/claude-code"
mkdir -p "$CLAUDE_CONFIG_DIR"

cat > "$CLAUDE_CONFIG_DIR/mcp.json" << EOF
{
  "mcpServers": {
    "local-llm": {
      "command": "python3",
      "args": ["$PROJECT_DIR/local_llm_mcp_server.py"],
      "env": {
        "MODEL_PATH": "$PROJECT_DIR/$MODEL_FILE",
        "N_GPU_LAYERS": "-1",
        "N_CTX": "8192",
        "N_BATCH": "512", 
        "N_THREADS": "8",
        "USE_MMAP": "True",
        "VERBOSE": "False",
        "JOBS_DIR": "$PROJECT_DIR/jobs",
        "CUDA_VISIBLE_DEVICES": "0"
      }
    }
  }
}
EOF

# Create test script
cat > test_server.py << 'EOF'
#!/usr/bin/env python3
"""Test script for the Local LLM MCP Server"""

import asyncio
import sys
import os

# Add the project directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from local_llm_mcp_server import LocalLLMServer

async def test_server():
    server = LocalLLMServer()
    
    print("üß™ Testing model loading...")
    if not server.load_model():
        print("‚ùå Model loading failed")
        return False
    
    print("‚úÖ Model loaded successfully!")
    
    # Test generation
    print("\nü§ñ Testing code generation...")
    test_args = {
        "prompt": "Write a simple Python function to reverse a string",
        "max_tokens": 256
    }
    
    result = await server.generate_code(test_args)
    print(f"üìù Generated response: {result.content[0].text[:200]}...")
    
    # Test model info
    print("\nüìä Getting model info...")
    info_result = await server.get_model_info()
    print(f"‚ÑπÔ∏è  Model info: {info_result.content[0].text[:200]}...")
    
    print("\n‚úÖ All tests passed!")
    return True

if __name__ == "__main__":
    asyncio.run(test_server())
EOF

chmod +x test_server.py

# Create start script
cat > start_server.sh << 'EOF'
#!/bin/bash
cd "$(dirname "$0")"
source venv/bin/activate
python3 local_llm_mcp_server.py "$@"
EOF

chmod +x start_server.sh

echo "üéâ Setup complete!"
echo ""
echo "üìÅ Project directory: $PROJECT_DIR"
echo "ü§ñ Model location: $PROJECT_DIR/$MODEL_FILE"
echo "‚öôÔ∏è  Claude Code config: $CLAUDE_CONFIG_DIR/mcp.json"
echo ""
echo "üß™ To test the server:"
echo "   cd $PROJECT_DIR && source venv/bin/activate && python3 test_server.py"
echo ""
echo "üöÄ To start the MCP server:"
echo "   cd $PROJECT_DIR && ./start_server.sh"
echo ""
echo "üîß Claude Code should now detect your local LLM server automatically!"
echo "   Use tools like 'generate_code', 'model_info', and 'benchmark_model'"

# Test GPU availability
echo ""
echo "üñ•Ô∏è  GPU Status:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits | while read gpu; do
        echo "   GPU: $gpu MB"
    done
else
    echo "   ‚ö†Ô∏è  NVIDIA GPU tools not available"
fi

# Memory recommendations for RTX 1080ti
echo ""
echo "üí° RTX 1080ti (11GB) Recommendations:"
echo "   - 7B model: Perfect fit with room for context"
echo "   - Context size: 8192 tokens (can increase to 16384 if needed)"
echo "   - Batch size: 512 (good balance of speed/memory)"
echo "   - All GPU layers: -1 (uses all 11GB effectively)"