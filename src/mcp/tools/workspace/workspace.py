"""Workspace Tool - File Operations with JSON Metadata Support

This tool handles all workspace file operations including reading, writing, and managing
files with JSON metadata generated by agents.
"""

import json
import logging
import os
import shutil
from pathlib import Path
from typing import Any, Dict, List

from src.core.utils.utils import create_mcp_response, handle_exception
from src.schemas.files.python_file import (
    PythonFile, PythonClass, PythonMethod, PythonFunction,
    PythonImport, PythonVariable, create_empty_python_file
)
from src.core.files.file_manager import FileManager

logger = logging.getLogger(__name__)


# Initialize file manager for jinja2 template rendering
def _get_file_manager(workspace_root: Path) -> FileManager:
    """Get FileManager instance for template rendering"""
    templates_path = Path("/app/templates")  # Fixed absolute path
    return FileManager(str(workspace_root), str(templates_path))


def _extract_imports_from_body(body: str) -> tuple[list[str], str]:
    """Extract import statements from method body and return (imports, cleaned_body)"""
    import re

    # Find import statements in the body
    import_patterns = [
        r'^(\s*)import\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*$',
        r'^(\s*)from\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s+import\s+(.+)\s*$'
    ]

    extracted_imports = []
    cleaned_lines = []

    # Pattern to detect method definition lines (def method_name(...):)
    method_def_pattern = r'^\s*def\s+[a-zA-Z_][a-zA-Z0-9_]*\s*\([^)]*\)\s*:\s*$'

    for line in body.split('\n'):
        is_import = False
        is_method_def = False

        # Check for method definition line
        if re.match(method_def_pattern, line):
            is_method_def = True

        # Check for regular import
        match = re.match(import_patterns[0], line)
        if match:
            module = match.group(2)
            extracted_imports.append(f"import {module}")
            is_import = True

        # Check for from...import
        match = re.match(import_patterns[1], line)
        if match:
            module = match.group(2)
            items = match.group(3)
            extracted_imports.append(f"from {module} import {items}")
            is_import = True

        # Only keep lines that are not imports or method definitions
        if not is_import and not is_method_def:
            cleaned_lines.append(line)

    cleaned_body = '\n'.join(cleaned_lines).strip()
    return extracted_imports, cleaned_body



def _json_to_python_file(json_obj: Dict[str, Any], filename: str) -> PythonFile:
    """Convert JSON structure to PythonFile schema object"""
    python_file = create_empty_python_file(filename)

    # Track imports extracted from method bodies
    extracted_imports = set()

    try:
        # Extract module docstring from metadata
        metadata = json_obj.get('metadata', {})
        if metadata and isinstance(metadata, dict):
            description = metadata.get('description', '')
            if description:
                python_file.module_docstring = description.strip()

        # Process imports
        imports = json_obj.get('imports', [])
        if imports and isinstance(imports, list):
            for import_data in imports:
                if isinstance(import_data, dict):
                    module = import_data.get('module', '')
                    items = import_data.get('items', '')
                    if module:
                        items_list = [item.strip() for item in items.split(',')] if items else []
                        import_obj = PythonImport(module=module, items=items_list)
                        python_file.add_import(import_obj)

        # Process constants as variables
        constants = json_obj.get('constants', [])
        if constants and isinstance(constants, list):
            for const_data in constants:
                if isinstance(const_data, dict):
                    name = const_data.get('name', '')
                    const_type = const_data.get('type', '')
                    value = const_data.get('value', '')
                    if name and value:
                        variable = PythonVariable(
                            name=name,
                            type_hint=const_type if const_type else None,
                            value=value
                        )
                        python_file.variables.append(variable)

        # Process functions
        functions = json_obj.get('functions', [])
        if functions and isinstance(functions, list):
            for func_data in functions:
                if isinstance(func_data, dict):
                    name = func_data.get('name', 'unknown_function')

                    # Extract parameters
                    parameters = []
                    params_data = func_data.get('parameters', [])
                    if params_data and isinstance(params_data, list):
                        for param_data in params_data:
                            if isinstance(param_data, dict):
                                param_name = param_data.get('name', '')
                                param_type = param_data.get('type', '')
                                param_default = param_data.get('default', '')

                                if param_name:
                                    param_dict = {"name": param_name}
                                    if param_type:
                                        param_dict["type"] = param_type
                                    if param_default:
                                        param_dict["default"] = param_default
                                    parameters.append(param_dict)

                    # Extract return type and body
                    return_type = None
                    returns_data = func_data.get('returns', {})
                    if returns_data and isinstance(returns_data, dict):
                        return_type = returns_data.get('type')

                    body = "pass"
                    raw_body = func_data.get('body', '')
                    if raw_body:
                        # Extract imports from function body
                        body_imports, cleaned_body = _extract_imports_from_body(raw_body.strip())
                        extracted_imports.update(body_imports)
                        body = cleaned_body if cleaned_body else "pass"

                    function = PythonFunction(
                        name=name,
                        docstring=None,
                        parameters=parameters,
                        return_type=return_type,
                        body=body
                    )
                    python_file.add_or_update_function(function)

        # Process classes
        classes = json_obj.get('classes', [])
        if classes and isinstance(classes, list):
            for class_data in classes:
                if isinstance(class_data, dict):
                    class_name = class_data.get('name', 'UnknownClass')

                    # Class docstring
                    docstring = class_data.get('docstring', '')

                    methods = []

                    # Process __init__ method
                    init_data = class_data.get('init_method', {})
                    if init_data and isinstance(init_data, dict):
                        init_params = []
                        params_data = init_data.get('parameters', [])
                        if params_data and isinstance(params_data, list):
                            for param_data in params_data:
                                if isinstance(param_data, dict):
                                    param_name = param_data.get('name', '')
                                    param_type = param_data.get('type', '')
                                    param_default = param_data.get('default', '')

                                    if param_name:
                                        param_dict = {"name": param_name}
                                        if param_type:
                                            param_dict["type"] = param_type
                                        if param_default:
                                            param_dict["default"] = param_default
                                        init_params.append(param_dict)

                        init_body = "pass"
                        raw_body = init_data.get('body', '')
                        if raw_body:
                            # Extract imports from method body
                            body_imports, cleaned_body = _extract_imports_from_body(raw_body.strip())
                            extracted_imports.update(body_imports)
                            init_body = cleaned_body if cleaned_body else "pass"

                        init_method = PythonMethod(
                            name="__init__",
                            docstring=None,
                            parameters=init_params,
                            return_type=None,
                            body=init_body
                        )
                        methods.append(init_method)

                    # Process other methods
                    methods_data = class_data.get('methods', [])
                    if methods_data and isinstance(methods_data, list):
                        for method_data in methods_data:
                            if isinstance(method_data, dict):
                                method_name = method_data.get('name', 'unknown_method')

                                # Extract parameters
                                method_params = []
                                params_data = method_data.get('parameters', [])
                                if params_data and isinstance(params_data, list):
                                    for param_data in params_data:
                                        if isinstance(param_data, dict):
                                            param_name = param_data.get('name', '')
                                            param_type = param_data.get('type', '')
                                            param_default = param_data.get('default', '')

                                            if param_name:
                                                param_dict = {"name": param_name}
                                                if param_type:
                                                    param_dict["type"] = param_type
                                                if param_default:
                                                    param_dict["default"] = param_default
                                                method_params.append(param_dict)

                                # Extract return type and body
                                method_return_type = None
                                returns_data = method_data.get('returns', {})
                                if returns_data and isinstance(returns_data, dict):
                                    method_return_type = returns_data.get('type')

                                method_body = "pass"
                                raw_body = method_data.get('body', '')
                                if raw_body:
                                    # Extract imports from method body
                                    body_imports, cleaned_body = _extract_imports_from_body(raw_body.strip())
                                    extracted_imports.update(body_imports)
                                    method_body = cleaned_body if cleaned_body else "pass"

                                method = PythonMethod(
                                    name=method_name,
                                    docstring=None,
                                    parameters=method_params,
                                    return_type=method_return_type,
                                    body=method_body
                                )
                                methods.append(method)

                    python_class = PythonClass(
                        name=class_name,
                        docstring=docstring,
                        base_classes=[],
                        methods=methods
                    )
                    python_file.add_or_update_class(python_class)

        # Add all extracted imports from method/function bodies to module level
        for import_stmt in extracted_imports:
            logger.info(f"ðŸ”„ Extracting import from method body: {import_stmt}")
            if import_stmt.startswith('import '):
                module = import_stmt[7:].strip()
                import_obj = PythonImport(module=module, items=[])
                python_file.add_import(import_obj)
            elif import_stmt.startswith('from '):
                # Parse "from module import items" format
                parts = import_stmt[5:].split(' import ')
                if len(parts) == 2:
                    module = parts[0].strip()
                    items = [item.strip() for item in parts[1].split(',')]
                    import_obj = PythonImport(module=module, items=items)
                    python_file.add_import(import_obj)

        return python_file

    except Exception as e:
        logger.error(f"Error converting JSON to PythonFile: {e}")
        raise


async def workspace_tool(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """Workspace operations tool

    Args:
        arguments: Tool arguments containing action and parameters

    Returns:
        MCP response dictionary
    """
    try:
        action = arguments.get("action")
        if not action:
            return create_mcp_response(False, "Missing 'action' parameter")

        # Get the workspace root (default to /workspace for Docker)
        workspace_root = Path(os.environ.get("WORKSPACE_ROOT", "/workspace"))

        if action == "read":
            return await _read_file(arguments, workspace_root)
        elif action == "delete":
            return await _delete_file(arguments, workspace_root)
        elif action == "list":
            return await _list_directory(arguments, workspace_root)
        elif action == "search":
            return await _search_files(arguments, workspace_root)
        elif action == "create_dir":
            return await _create_directory(arguments, workspace_root)
        elif action == "tree":
            return await _show_tree(arguments, workspace_root)
        elif action == "generate_from_metadata":
            return await _generate_from_metadata(arguments, workspace_root)
        else:
            return create_mcp_response(False, f"Unknown workspace action: {action}")

    except Exception as e:
        return handle_exception(e, "workspace_tool")


async def _read_file(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Read file content"""
    try:
        file_path = arguments.get("path")
        if not file_path:
            return create_mcp_response(False, "Missing 'path' parameter for read action")

        full_path = workspace_root / file_path

        if not full_path.exists():
            return create_mcp_response(False, f"File does not exist: {file_path}")

        if not full_path.is_file():
            return create_mcp_response(False, f"Path is not a file: {file_path}")

        # Check if it's a binary file
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            return create_mcp_response(False, f"Cannot read binary file: {file_path}")

        return create_mcp_response(True, f"Read {len(content)} characters from {file_path}", {
            "path": file_path,
            "content": content,
            "size": len(content)
        })

    except Exception as e:
        return handle_exception(e, "_read_file")




async def _process_file_metadata(file_path: str, workspace_root: Path) -> None:
    """Process agent JSON metadata files for the given file

    This function looks for corresponding .meta JSON files and processes them
    to enhance the generated code with component tracking.
    """
    try:
        meta_dir = workspace_root / ".meta"

        # Check for JSON metadata
        json_meta_file = meta_dir / f"{file_path}.json"

        if json_meta_file.exists():
            logger.info(f"Found JSON metadata for {file_path}, processing...")
            await _process_json_metadata(file_path, json_meta_file, workspace_root)
        else:
            logger.debug(f"No metadata found for {file_path}")

    except Exception as e:
        logger.error(f"Error processing metadata for {file_path}: {e}")




async def _process_json_metadata(file_path: str, json_meta_file: Path, workspace_root: Path) -> None:
    """Process JSON metadata file (backward compatibility)"""
    try:
        import json

        with open(json_meta_file, 'r') as f:
            metadata = json.load(f)

        logger.info(f"Processed JSON metadata for {file_path}: {metadata.get('description', 'No description')}")

    except Exception as e:
        logger.error(f"Error processing JSON metadata {json_meta_file}: {e}")


async def _add_component_tracking(file_path: str, components: List[Dict[str, Any]], workspace_root: Path) -> None:
    """Add component tracking comments to the generated file"""
    try:
        full_path = workspace_root / file_path

        if not full_path.exists():
            return

        with open(full_path, 'r') as f:
            content = f.read()

        # Add component tracking header
        tracking_header = "# Component tracking enabled by agent metadata\n"
        tracking_header += f"# Components: {', '.join([c['name'] for c in components])}\n\n"

        enhanced_content = tracking_header + content

        with open(full_path, 'w') as f:
            f.write(enhanced_content)

        logger.info(f"Added component tracking to {file_path}")

    except Exception as e:
        logger.error(f"Error adding component tracking to {file_path}: {e}")


async def _delete_file(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Delete file or directory"""
    try:
        file_path = arguments.get("path")
        if not file_path:
            return create_mcp_response(False, "Missing 'path' parameter for delete action")

        full_path = workspace_root / file_path

        if not full_path.exists():
            return create_mcp_response(False, f"Path does not exist: {file_path}")

        if full_path.is_file():
            full_path.unlink()
            return create_mcp_response(True, f"Successfully deleted file: {file_path}")
        elif full_path.is_dir():
            shutil.rmtree(full_path)
            return create_mcp_response(True, f"Successfully deleted directory: {file_path}")
        else:
            return create_mcp_response(False, f"Unknown path type: {file_path}")

    except Exception as e:
        return handle_exception(e, "_delete_file")


async def _list_directory(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """List directory contents"""
    try:
        dir_path = arguments.get("path", ".")
        include_hidden = arguments.get("include_hidden", False)
        recursive = arguments.get("recursive", False)

        full_path = workspace_root / dir_path

        if not full_path.exists():
            return create_mcp_response(False, f"Directory does not exist: {dir_path}")

        if not full_path.is_dir():
            return create_mcp_response(False, f"Path is not a directory: {dir_path}")

        files = []
        if recursive:
            for item in full_path.rglob("*"):
                if not include_hidden and item.name.startswith('.'):
                    continue
                relative_path = item.relative_to(workspace_root)
                files.append({
                    "path": str(relative_path),
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })
        else:
            for item in full_path.iterdir():
                if not include_hidden and item.name.startswith('.'):
                    continue
                relative_path = item.relative_to(workspace_root)
                files.append({
                    "path": str(relative_path),
                    "type": "file" if item.is_file() else "directory",
                    "size": item.stat().st_size if item.is_file() else None
                })

        return create_mcp_response(True, f"Listed {len(files)} items in {dir_path}", {
            "path": dir_path,
            "files": files,
            "count": len(files)
        })

    except Exception as e:
        return handle_exception(e, "_list_directory")


async def _search_files(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Search for files matching pattern"""
    try:
        pattern = arguments.get("pattern", "*")
        file_pattern = arguments.get("file_pattern", "*.py")
        search_path = arguments.get("path", ".")

        full_path = workspace_root / search_path

        if not full_path.exists():
            return create_mcp_response(False, f"Search path does not exist: {search_path}")

        matches = []
        for item in full_path.rglob(file_pattern):
            if item.is_file():
                relative_path = item.relative_to(workspace_root)
                matches.append({
                    "path": str(relative_path),
                    "size": item.stat().st_size
                })

        return create_mcp_response(True, f"Found {len(matches)} files matching pattern", {
            "pattern": file_pattern,
            "matches": matches,
            "count": len(matches)
        })

    except Exception as e:
        return handle_exception(e, "_search_files")


async def _create_directory(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Create directory"""
    try:
        dir_path = arguments.get("path")
        if not dir_path:
            return create_mcp_response(False, "Missing 'path' parameter for create_dir action")

        full_path = workspace_root / dir_path
        full_path.mkdir(parents=True, exist_ok=True)

        return create_mcp_response(True, f"Successfully created directory: {dir_path}", {
            "path": dir_path
        })

    except Exception as e:
        return handle_exception(e, "_create_directory")


async def _show_tree(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Show directory tree structure"""
    try:
        root_path = arguments.get("path", ".")
        max_depth = arguments.get("max_depth", 3)
        include_hidden = arguments.get("include_hidden", False)

        full_path = workspace_root / root_path

        if not full_path.exists():
            return create_mcp_response(False, f"Path does not exist: {root_path}")

        tree_structure = _build_tree_structure(full_path, workspace_root, max_depth, include_hidden)

        return create_mcp_response(True, f"Directory tree for {root_path}", {
            "path": root_path,
            "tree": tree_structure
        })

    except Exception as e:
        return handle_exception(e, "_show_tree")


def _build_tree_structure(path: Path, workspace_root: Path, max_depth: int, include_hidden: bool, current_depth: int = 0) -> List[Dict[str, Any]]:
    """Build tree structure recursively"""
    if current_depth >= max_depth:
        return []

    items = []
    try:
        for item in sorted(path.iterdir()):
            if not include_hidden and item.name.startswith('.'):
                continue

            relative_path = item.relative_to(workspace_root)
            item_dict = {
                "name": item.name,
                "path": str(relative_path),
                "type": "file" if item.is_file() else "directory"
            }

            if item.is_file():
                item_dict["size"] = item.stat().st_size
            elif item.is_dir():
                item_dict["children"] = _build_tree_structure(
                    item, workspace_root, max_depth, include_hidden, current_depth + 1
                )

            items.append(item_dict)

    except PermissionError:
        # Skip directories we can't read
        pass

    return items






async def _generate_from_metadata(arguments: Dict[str, Any], workspace_root: Path) -> Dict[str, Any]:
    """Generate Python file from existing JSON metadata file using jinja2 template"""
    try:
        path = arguments.get("path")
        if not path:
            return create_mcp_response(False, "Missing 'path' parameter")

        # Ensure path is relative
        if path.startswith('/'):
            path = path[1:]

        # Check if metadata file exists
        meta_file = workspace_root / ".meta" / f"{path}.json"
        if not meta_file.exists():
            return create_mcp_response(False, f"Metadata file not found: {meta_file}")

        logger.info(f"ðŸ“– Reading metadata from: {meta_file}")

        # Read JSON metadata
        with open(meta_file, 'r', encoding='utf-8') as f:
            json_content = f.read()

        # Parse JSON to extract structured content
        try:
            import json
            metadata_obj = json.loads(json_content)

            if not isinstance(metadata_obj, dict):
                return create_mcp_response(False, f"Invalid metadata: root must be an object, got {type(metadata_obj)}")

            logger.info(f"ðŸ”„ Converting JSON to PythonFile schema for {path}")

            # Convert JSON to PythonFile schema object
            python_file_obj = _json_to_python_file(metadata_obj, path)

            logger.info(f"ðŸŽ¨ Rendering Python code using jinja2 template for {path}")

            # Use existing JSONFileManager to render via jinja2
            file_manager = _get_file_manager(workspace_root)

            # Render using the existing, working jinja2 system
            template = file_manager.jinja_env.get_template("python_file.j2")
            # Pass the PythonFile object directly, not converted to dict
            # This preserves methods like to_import_statement()
            template_data = {
                'module_docstring': python_file_obj.module_docstring,
                'imports': python_file_obj.imports,  # Keep PythonImport objects
                'variables': python_file_obj.variables,
                'classes': python_file_obj.classes,
                'functions': python_file_obj.functions,
                'dataclasses': python_file_obj.dataclasses
            }
            python_code = template.render(template_data)

            logger.info(f"âœ… Generated {len(python_code)} characters of Python code")

            # Write the generated Python file
            target_file = workspace_root / path
            target_file.parent.mkdir(parents=True, exist_ok=True)

            with open(target_file, 'w', encoding='utf-8') as f:
                f.write(python_code)

            logger.info(f"âœ… Generated Python file: {target_file}")

            return create_mcp_response(
                True,
                f"âœ… Successfully generated {path} from metadata ({len(python_code)} chars)",
                {
                    "path": path,
                    "metadata_file": str(meta_file),
                    "generated_file": str(target_file),
                    "size": len(python_code)
                }
            )

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error in metadata file {meta_file}: {e}")
            return create_mcp_response(False, f"Invalid JSON in metadata file: {e}")

        except Exception as e:
            logger.error(f"Jinja2 template rendering error for {path}: {e}")
            return create_mcp_response(False, f"Template rendering failed: {e}")

    except Exception as e:
        logger.error(f"Generate from metadata error: {e}")
        return handle_exception(e, "_generate_from_metadata")