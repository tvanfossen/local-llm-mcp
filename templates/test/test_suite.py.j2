{# 
Template: Comprehensive Test Suite
Description: Test suite template with fixtures, mocks, and comprehensive coverage
Variables: module_name, test_class_name, functions_to_test
#}
"""Test Suite for {{ module_name|title }}

Comprehensive test coverage including:
- Unit tests for all functions
- Integration tests
- Error handling tests
- Edge case coverage
- Performance tests

Generated from template on {{ generation_date }}
"""

import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from typing import Any, Dict, List
{% if test_framework == "pytest-asyncio" %}
import pytest_asyncio
{% endif %}

# Import the module under test
{% if import_path %}
from {{ import_path }} import {{ functions_to_test|join(', ') }}
{% else %}
from {{ module_name }} import {{ functions_to_test|join(', ') }}
{% endif %}


class {{ test_class_name }}:
    """Test suite for {{ module_name }} module"""
    
    @pytest.fixture
    def sample_args(self) -> Dict[str, Any]:
        """Sample arguments for testing"""
        return {
            {% for arg, value in sample_arguments.items() %}
            "{{ arg }}": {{ value|tojson }},
            {% endfor %}
        }
    
    {% if database_tests %}
    @pytest.fixture
    def mock_database(self):
        """Mock database connection"""
        with patch('{{ module_name }}._get_database_connection') as mock_db:
            mock_connection = Mock()
            mock_cursor = Mock()
            mock_connection.cursor.return_value = mock_cursor
            mock_db.return_value = mock_connection
            yield {
                'connection': mock_connection,
                'cursor': mock_cursor
            }
    {% endif %}
    
    {% if file_system_tests %}
    @pytest.fixture
    def mock_file_system(self, tmp_path):
        """Mock file system operations"""
        test_dir = tmp_path / "test_workspace"
        test_dir.mkdir()
        
        with patch('{{ module_name }}.ConfigManager') as mock_config:
            mock_config.return_value.system.get_workspace_root.return_value = test_dir
            yield test_dir
    {% endif %}
    
    {% if agent_tests %}
    @pytest.fixture
    def mock_agent(self):
        """Mock agent for testing"""
        agent = Mock()
        agent.state.agent_id = "test-agent-123"
        agent.state.name = "Test Agent"
        agent.state.interaction_count = 0
        agent.state.successful_operations = 0
        agent.state.failed_operations = 0
        agent.managed_files = ["test_file.py"]
        agent.conversation_history = []
        agent._get_timestamp.return_value = "2024-01-01T00:00:00Z"
        return agent
    {% endif %}
    
    {% for function in functions_to_test %}
    
    # ============================================================================
    # Tests for {{ function }}
    # ============================================================================
    
    {% if function.endswith('_handler') or 'async' in function %}
    @pytest.mark.asyncio
    async def test_{{ function }}_success(self, sample_args{% if database_tests %}, mock_database{% endif %}{% if file_system_tests %}, mock_file_system{% endif %}{% if agent_tests %}, mock_agent{% endif %}):
        """Test successful {{ function }} execution"""
        {% if agent_tests %}
        # Setup agent registry mock
        with patch('{{ module_name }}.AgentRegistry') as mock_registry:
            mock_registry.return_value.get_agent.return_value = mock_agent
            
            result = await {{ function }}(sample_args)
        {% else %}
        result = await {{ function }}(sample_args)
        {% endif %}
        
        assert result is not None
        assert result["isError"] is False
        assert "content" in result
        assert len(result["content"]) > 0
        assert result["content"][0]["type"] == "text"
        
        # Check success indicators in text
        response_text = result["content"][0]["text"]
        assert "✅" in response_text or "successful" in response_text.lower()
    
    @pytest.mark.asyncio
    async def test_{{ function }}_missing_required_params(self):
        """Test {{ function }} with missing required parameters"""
        empty_args = {}
        result = await {{ function }}(empty_args)
        
        assert result["isError"] is True
        response_text = result["content"][0]["text"]
        assert "Missing Parameter" in response_text
    
    @pytest.mark.asyncio
    async def test_{{ function }}_invalid_params(self, sample_args):
        """Test {{ function }} with invalid parameters"""
        # Test with invalid parameter values
        invalid_args = sample_args.copy()
        {% if function.endswith('_handler') %}
        invalid_args["agent_id"] = "invalid-agent-id"
        {% endif %}
        invalid_args["invalid_param"] = None
        
        {% if agent_tests %}
        with patch('{{ module_name }}.AgentRegistry') as mock_registry:
            mock_registry.return_value.get_agent.return_value = None
            
            result = await {{ function }}(invalid_args)
        {% else %}
        result = await {{ function }}(invalid_args)
        {% endif %}
        
        assert result["isError"] is True
        response_text = result["content"][0]["text"]
        assert "❌" in response_text
    
    @pytest.mark.asyncio
    async def test_{{ function }}_exception_handling(self, sample_args):
        """Test {{ function }} exception handling"""
        {% if database_tests %}
        with patch('{{ module_name }}._get_database_connection', side_effect=Exception("Database error")):
            result = await {{ function }}(sample_args)
        {% elif file_system_tests %}
        with patch('pathlib.Path.exists', side_effect=Exception("File system error")):
            result = await {{ function }}(sample_args)
        {% else %}
        with patch('{{ module_name }}.logger') as mock_logger:
            # Mock an internal function to raise an exception
            with patch('{{ module_name }}._internal_function', side_effect=Exception("Test error")):
                result = await {{ function }}(sample_args)
        {% endif %}
        
        assert result["isError"] is True
        response_text = result["content"][0]["text"]
        assert "Error" in response_text
    
    {% else %}
    def test_{{ function }}_success(self, sample_args{% if database_tests %}, mock_database{% endif %}{% if file_system_tests %}, mock_file_system{% endif %}):
        """Test successful {{ function }} execution"""
        result = {{ function }}(sample_args)
        
        assert result is not None
        assert result["isError"] is False
        assert "content" in result
    
    def test_{{ function }}_missing_params(self):
        """Test {{ function }} with missing parameters"""
        result = {{ function }}({})
        assert result["isError"] is True
    
    {% endif %}
    
    {% if performance_tests %}
    @pytest.mark.performance
    {% if function.endswith('_handler') or 'async' in function %}
    @pytest.mark.asyncio
    async def test_{{ function }}_performance(self, sample_args):
        """Test {{ function }} performance"""
        import time
        
        start_time = time.time()
        result = await {{ function }}(sample_args)
        end_time = time.time()
        
        execution_time = end_time - start_time
        assert execution_time < {{ performance_threshold|default(5.0) }}  # Should complete within 5 seconds
        assert result["isError"] is False
    {% else %}
    def test_{{ function }}_performance(self, sample_args):
        """Test {{ function }} performance"""
        import time
        
        start_time = time.time()
        result = {{ function }}(sample_args)
        end_time = time.time()
        
        execution_time = end_time - start_time
        assert execution_time < {{ performance_threshold|default(1.0) }}  # Should complete within 1 second
        assert result["isError"] is False
    {% endif %}
    {% endif %}
    
    {% endfor %}
    
    # ============================================================================
    # Integration Tests
    # ============================================================================
    
    {% if integration_tests %}
    @pytest.mark.integration
    {% if functions_to_test|length > 1 %}
    @pytest.mark.asyncio
    async def test_integration_workflow(self, sample_args):
        """Test integration between multiple functions"""
        # Test a complete workflow using multiple functions
        {% for function in functions_to_test[:3] %}
        {% if function.endswith('_handler') or 'async' in function %}
        result_{{ loop.index }} = await {{ function }}(sample_args)
        assert result_{{ loop.index }}["isError"] is False
        {% endif %}
        {% endfor %}
        
        # Verify workflow completed successfully
        # Add specific integration assertions here
    {% endif %}
    
    @pytest.mark.integration
    def test_configuration_integration(self):
        """Test integration with configuration management"""
        from src.core.config.manager.manager import ConfigManager
        
        config = ConfigManager()
        assert config is not None
        assert config.system.workspace_root.exists()
    
    {% if database_tests %}
    @pytest.mark.integration
    def test_database_integration(self, mock_database):
        """Test database integration"""
        db = mock_database
        
        # Test database connection
        assert db['connection'] is not None
        assert db['cursor'] is not None
        
        # Test basic database operations
        db['cursor'].execute.return_value = None
        db['cursor'].fetchone.return_value = ("test_result",)
        
        # Verify mocks work as expected
        db['cursor'].execute("SELECT 1")
        result = db['cursor'].fetchone()
        assert result == ("test_result",)
    {% endif %}
    {% endif %}
    
    # ============================================================================
    # Edge Case Tests
    # ============================================================================
    
    {% if edge_case_tests %}
    {% for function in functions_to_test %}
    {% if function.endswith('_handler') or 'async' in function %}
    @pytest.mark.asyncio
    async def test_{{ function }}_edge_cases(self):
        """Test {{ function }} edge cases"""
        edge_cases = [
            {},  # Empty args
            {"invalid_key": "invalid_value"},  # Invalid args
            {% if edge_case_data %}
            {% for case in edge_case_data %}
            {{ case|tojson }},
            {% endfor %}
            {% endif %}
        ]
        
        for case in edge_cases:
            result = await {{ function }}(case)
            assert "content" in result
            assert isinstance(result["isError"], bool)
    {% endif %}
    {% endfor %}
    {% endif %}
    
    # ============================================================================
    # Helper Function Tests
    # ============================================================================
    
    {% if helper_functions %}
    {% for helper in helper_functions %}
    def test_{{ helper.name }}(self):
        """Test {{ helper.name }} helper function"""
        # TODO: Add specific tests for {{ helper.name }}
        pass
    {% endfor %}
    {% endif %}
    
    # ============================================================================
    # Cleanup and Utilities
    # ============================================================================
    
    def test_response_format_consistency(self, sample_args):
        """Test that all functions return consistent response format"""
        {% for function in functions_to_test %}
        {% if function.endswith('_handler') or 'async' in function %}
        # Test {{ function }}
        result = asyncio.run({{ function }}(sample_args))
        {% else %}
        result = {{ function }}(sample_args)
        {% endif %}
        
        # Verify standard MCP response format
        assert isinstance(result, dict)
        assert "content" in result
        assert "isError" in result
        assert isinstance(result["content"], list)
        assert isinstance(result["isError"], bool)
        
        if result["content"]:
            assert "type" in result["content"][0]
            assert "text" in result["content"][0]
            assert result["content"][0]["type"] == "text"
        
        {% endfor %}
    
    {% if cleanup_tests %}
    def test_resource_cleanup(self):
        """Test proper resource cleanup"""
        # Test that resources are properly cleaned up after operations
        # This is especially important for database connections, file handles, etc.
        pass
    {% endif %}


# ============================================================================
# Parametrized Tests
# ============================================================================

{% if parametrized_tests %}
@pytest.mark.parametrize("test_input,expected", [
    {% for input_val, expected_val in parametrized_test_data %}
    ({{ input_val|tojson }}, {{ expected_val|tojson }}),
    {% endfor %}
])
def test_parametrized_scenarios(test_input, expected):
    """Test various input scenarios"""
    # TODO: Implement parametrized test logic
    pass
{% endif %}


# ============================================================================
# Test Configuration and Markers
# ============================================================================

# Custom pytest markers for this test suite
pytest_plugins = ["pytest_asyncio"]

# Test categories
pytestmark = [
    pytest.mark.{{ module_name.replace('_', '') }},
    {% if test_categories %}
    {% for category in test_categories %}
    pytest.mark.{{ category }},
    {% endfor %}
    {% endif %}
]